{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "executionInfo": {
     "elapsed": 1891328,
     "status": "ok",
     "timestamp": 1709959040064,
     "user": {
      "displayName": "Rohit Kohli",
      "userId": "13839515819669865697"
     },
     "user_tz": -330
    },
    "id": "WZZ_zoFQWiYD",
    "outputId": "13d5908a-5ce6-4171-97f8-38831c3b2190"
   },
   "outputs": [],
   "source": [
    "filepath = 'news.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 792,
     "status": "ok",
     "timestamp": 1709960669890,
     "user": {
      "displayName": "Rohit Kohli",
      "userId": "13839515819669865697"
     },
     "user_tz": -330
    },
    "id": "1lijKVgQY2w7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1709960675155,
     "user": {
      "displayName": "Rohit Kohli",
      "userId": "13839515819669865697"
     },
     "user_tz": -330
    },
    "id": "SAD31ePrgkcH",
    "outputId": "2af168f3-9faa-49f7-e337-836a68387be4"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text combining.\n",
    "df['NewsText'] = df['title'].fillna('') + \" \"+df['text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1709960675874,
     "user": {
      "displayName": "Rohit Kohli",
      "userId": "13839515819669865697"
     },
     "user_tz": -330
    },
    "id": "FjKH7ighgnKs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1709960675874,
     "user": {
      "displayName": "Rohit Kohli",
      "userId": "13839515819669865697"
     },
     "user_tz": -330
    },
    "id": "ToAR2Hc3iVLG",
    "outputId": "e9992be0-6999-473f-ff75-e320d4813fbf"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1709960675874,
     "user": {
      "displayName": "Rohit Kohli",
      "userId": "13839515819669865697"
     },
     "user_tz": -330
    },
    "id": "ztKLV2PZiuLt",
    "outputId": "03aa3918-0ac4-426a-82dd-8528b1805ae9"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1709961935604,
     "user": {
      "displayName": "Rohit Kohli",
      "userId": "13839515819669865697"
     },
     "user_tz": -330
    },
    "id": "DG1RA_OwmopR"
   },
   "outputs": [],
   "source": [
    "# Cleaning Function\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # lowercase\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)  # remove URLs\n",
    "    text = re.sub(r'<.*?>', '', text)  # remove HTML tags\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)  # remove emails\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # remove digits & punctuations\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # remove extra spaces\n",
    "\n",
    "    # Tokenization + Stopword removal + Lemmatization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NewsContent'] = df['NewsText'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now dropping the Uncessary Columns\n",
    "df = df.drop(['Unnamed: 0','title','text','NewsText'],axis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_embeddings(text, tokenizer, model, max_length=128, device='cuda'):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=max_length,\n",
    "                       truncation=True, padding=True).to(device)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()  # Makes sure model runs in inference mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "    return cls_embedding.cpu().numpy()  # Move result back to CPU for numpy\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for text in df['NewsContent']:\n",
    "    emb = bert_embeddings(text, tokenizer, model)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n",
    "print(\"Torch CUDA version:\", torch.version.cuda)\n",
    "print(\"Torch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 514,
     "status": "ok",
     "timestamp": 1709962284663,
     "user": {
      "displayName": "Rohit Kohli",
      "userId": "13839515819669865697"
     },
     "user_tz": -330
    },
    "id": "lrRiyRQ5sAHP"
   },
   "outputs": [],
   "source": [
    "x = embeddings\n",
    "y = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "classes = np.array([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "if hasattr(y, 'values'):\n",
    "    y = y.values\n",
    "\n",
    "classes = np.unique(y)\n",
    "print(f\"Found {len(classes)} classes: {classes}\")\n",
    "\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y)\n",
    "print(f\"Class weights: {weights}\")\n",
    "\n",
    "class_weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,stratify=y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"xgbc\":XGBClassifier(use_label_encoder=False,eval_metric='logloss'),\n",
    "    \"cbc\":CatBoostClassifier(verbose=0),\n",
    "    \"lgbm\":LGBMClassifier()\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    \"xgbc\": {\n",
    "        \"n_estimators\": [20,50,100,200],\n",
    "        \"learning_rate\": [0.01, 0.1],\n",
    "        \"max_depth\": [3, 6],\n",
    "        \"subsample\": [0.8],  \n",
    "        \"subsample\": [0.8],  \n",
    "    },\n",
    "    \"cbc\":{\n",
    "        \"iterations\": [20,50,100,200],\n",
    "        \"learning_rate\": [0.01, 0.1],\n",
    "        \"depth\": [4, 6],\n",
    "        \"l2_leaf_reg\": [3, 5],\n",
    "        \"subsample\": [0.8]   \n",
    "    },\n",
    "    \"lgbm\":{\n",
    "        \"n_estimators\": [20,50,100,200],\n",
    "        \"learning_rate\": [0.01, 0.1],\n",
    "        \"num_leaves\": [10, 21],\n",
    "        \"max_depth\": [5, 7],\n",
    "        \"subsample\": [0.8],\n",
    "        \"colsample_bytree\": [0.8]  \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_memory_params = {\n",
    "    \"tree_method\": \"hist\",  \n",
    "    \"grow_policy\": \"lossguide\",  \n",
    "    \"max_bin\": 256,  \n",
    "    \"single_precision_histogram\": True  \n",
    "}\n",
    "\n",
    "# Add to your model definition\n",
    "models[\"xgbc\"].set_params(**xgb_memory_params)\n",
    "\n",
    "lgbm_memory_params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"device_type\": \"cpu\",  \n",
    "    \"max_bin\": 255,  \n",
    "    \"bin_construct_sample_cnt\": 20000  \n",
    "}\n",
    "\n",
    "models[\"lgbm\"].set_params(**lgbm_memory_params)\n",
    "\n",
    "catboost_memory_params = {\n",
    "    \"bootstrap_type\": \"Bernoulli\",  \n",
    "    \"subsample\": 0.8,  \n",
    "    \"used_ram_limit\": \"3gb\"  \n",
    "}\n",
    "\n",
    "models[\"cbc\"].set_params(**catboost_memory_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "scorer = make_scorer(f1_score, average='weighted')  \n",
    "\n",
    "models[\"xgbc\"].set_params(tree_method=\"hist\", grow_policy=\"lossguide\")\n",
    "models[\"lgbm\"].set_params(verbose=-1, device_type=\"cpu\")  \n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {name}...\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    try:\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_grid[name],\n",
    "            n_iter=3,  \n",
    "            scoring=scorer,\n",
    "            cv=2,  \n",
    "            random_state=42,\n",
    "            verbose=1,\n",
    "            n_jobs=1,   \n",
    "            error_score='raise'  \n",
    "        )\n",
    "        \n",
    "        search.fit(x_train, y_train)\n",
    "        best_models[name] = {\n",
    "            \"best_estimator\": search.best_estimator_,\n",
    "            \"best_score\": search.best_score_,\n",
    "            \"best_params\": search.best_params_\n",
    "        }\n",
    "        \n",
    "        print(f\"Completed {name} with best score: {search.best_score_:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error training {name}: {str(e)}\")\n",
    "        try:\n",
    "            print(f\"Trying simpler parameters for {name}...\")\n",
    "            simple_param_grid = {k: [v[0]] for k, v in param_grid[name].items()}   \n",
    "            simple_param_grid['n_estimators'] = [50]   \n",
    "            \n",
    "            search = RandomizedSearchCV(\n",
    "                estimator=model,\n",
    "                param_distributions=simple_param_grid,\n",
    "                n_iter=1,   \n",
    "                scoring=scorer,\n",
    "                cv=2,\n",
    "                random_state=42,\n",
    "                verbose=1,\n",
    "                n_jobs=1\n",
    "            )\n",
    "            \n",
    "            search.fit(x_train, y_train)\n",
    "            best_models[name] = {\n",
    "                \"best_estimator\": search.best_estimator_,\n",
    "                \"best_score\": search.best_score_,\n",
    "                \"best_params\": search.best_params_\n",
    "            }\n",
    "            \n",
    "            print(f\"Completed {name} with simple parameters. Score: {search.best_score_:.4f}\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"Failed to train {name} even with simple parameters: {str(e2)}\")\n",
    "            best_models[name] = {\n",
    "                \"best_estimator\": None,\n",
    "                \"best_score\": 0,\n",
    "                \"best_params\": {},\n",
    "                \"error\": str(e2)\n",
    "            }\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "for name, result in best_models.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    if result[\"best_estimator\"] is not None:\n",
    "        print(f\"Best F1 Score: {result['best_score']:.4f}\")\n",
    "        print(f\"Best Params: {result['best_params']}\")\n",
    "    else:\n",
    "        print(f\"Failed to train. Error: {result.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMClassifier(subsample=0.8,num_leaves=10,n_estimators=100,learning_rate=0.1,colsample_bytree=0.8,class_weight='balanced',random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = lgbm.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = f1_score(predict,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "try:\n",
    "    joblib.dump(lgbm, 'lightgbm.pkl')\n",
    "    print(\"Model lightgbm.pkl'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving files: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOwrpMlNm5rnRyGp11/Swm+",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
